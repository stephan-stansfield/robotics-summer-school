{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXjTloDwlR9q"
      },
      "source": [
        "##Example segmentation of gastrointestinal polyp images using u-nets and variants in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data used in this notebook is obtained from the   [**Kvasir SEG**](https://datasets.simula.no/kvasir-seg/) dataset. It is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated and verified by an experienced gastroenterologist.\n",
        "\n",
        "####**Description from the original source**\n",
        "\n",
        "*The human gastrointestinal (GI) tract is made up of different sections, one of them being the large bowel. Several types of anomalies and diseases can affect the large bowel, such as colorectal cancer. Colorectal cancer is the second most common cancer type among women and third most common among men. Polyps are precursors to colorectal cancer, and is found in nearly half of the individuals at age 50 having a screening colonoscopy, and are increasing with age. Colonoscopy is the gold standard for detection and assessment of these polyps with subsequent biopsy and removal of the polyps. Early disease detection has a huge impact on survival from colorectal cancer, and polyp detection is therefore important. In addition, several studies have shown that polyps are often overlooked during colonoscopies, with polyp miss rates of 14%-30% depending on the type and size of the polyps. Increasing the detection of polyps has been shown to decrease risk of colorectal cancer. Thus, automatic detection of more polyps at an early stage can play a crucial role in improving both prevention of and survival from colorectal cancer. This is the main motivation behind the development of a Kvasir-SEG dataset.*\n",
        "\n",
        "**Kvasir-SEG Dataset Details**\n",
        "\n",
        "The Kvasir-SEG dataset (size 46.2 MB) contains 1000 polyp images and their corresponding ground truth from the Kvasir Dataset v2. The resolution of the images contained in Kvasir-SEG varies from 332x487 to 1920x1072 pixels. The images and its corresponding masks are stored in two separate folders with the same filename. The image files are encoded using JPEG compression, and online browsing is facilitated. The open-access dataset can be easily downloaded for research and educational purposes.\n",
        "\n",
        "The bounding box (coordinate points) for the corresponding images are stored in a JSON file. This dataset is designed to push the state of the art solution for the polyp detection task.\n"
      ],
      "metadata": {
        "id": "kOFP9LVrjJYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modifications in the notebook version\n",
        "For the purpose of this notebook all images have been downsampled to 512x512 pixels and bounding boxes adjusted accordingly. Images with superimposed ground truth segmentation masks and bounding boxes have been created for illustration purposes. The modified version of the data can be imported directly into the notebook by running the cell below. The modified dataset has been split into predefined training-/validation-/test splits with 80%, 10% and 10% of the data in the respective splits. The resulting size of the dataset i somewhat larger, so downloading may take a few minutes."
      ],
      "metadata": {
        "id": "UseCb6OrmBJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSF5da89486T",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Get the data\n",
        "import os\n",
        "\n",
        "if os.path.isdir('kvasir_polyps') == False:\n",
        "\n",
        "  !curl -X GET -u 'jkha:szZbB-wJDwQ-DNxD3-KgggK-XRq7M' 'https://nextcloud.sdu.dk/remote.php/webdav/Introduction_to_medical_robotics/kvasir_polyps.zip' -o 'kvasir_polyps.zip'\n",
        "\n",
        "  !unzip 'kvasir_polyps.zip'\n",
        "  os.remove('kvasir_polyps.zip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL82ewXDzw_W"
      },
      "outputs": [],
      "source": [
        "#print tree to get an idea about the structure of the data\n",
        "for root_,dir_,file_ in os.walk('kvasir_polyps'):\n",
        "  print(root_,dir_,file_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QMwstY52O7r"
      },
      "outputs": [],
      "source": [
        "#explore the data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "train_data = 'kvasir_polyps/train'\n",
        "val_data = 'kvasir_polyps/val'\n",
        "test_data = 'kvasir_polyps/test'\n",
        "\n",
        "print('Train data')\n",
        "print('Images: ', len(os.listdir(f'{train_data}/images')))\n",
        "print('Masks: ',len(os.listdir(f'{train_data}/masks')))\n",
        "print('\\n')\n",
        "print('Val data')\n",
        "print('Images: ',len(os.listdir(f'{val_data}/images')))\n",
        "print('Masks: ', len(os.listdir(f'{val_data}/masks')))\n",
        "print('\\n')\n",
        "print('Test data')\n",
        "print('Images: ',len(os.listdir(f'{test_data}/images')))\n",
        "print('Masks: ',len(os.listdir(f'{test_data}/masks')))\n",
        "print('\\n')\n",
        "\n",
        "#plot some example images\n",
        "\n",
        "sample_files = random.choices(os.listdir(f'{val_data}/images'),k=3)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "columns = 1\n",
        "rows = 3\n",
        "\n",
        "print('Random example images from validation set')\n",
        "for i in range(1, (columns*rows)+1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    im = Image.open(f'{val_data}/images/{sample_files[i-1].split(\".\")[0]}.jpg')\n",
        "    msk = Image.open(f'{val_data}/masks/{sample_files[i-1].split(\".\")[0]}.png')\n",
        "    ovl = Image.open(f'{val_data}/overlays/{sample_files[i-1].split(\".\")[0]}.png')\n",
        "    #the label file is similar to the mask except the values for background/foreground are [0,1] rather than [0,255]\n",
        "    #we use the labels rather than mask to train the models later\n",
        "    lbl = Image.open(f'{val_data}/labels/{sample_files[i-1].split(\".\")[0]}.png')\n",
        "    im = np.concatenate([im,msk,ovl,np.array(lbl)*50],axis=1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(im)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bounding box annotations\n",
        "As mentioned in the describtion the images are also annotated with bounding boxes for individual polyps. The bounding boxes will not be used in this notebook, but to get an idea of how these annotations are stored an example of obtaining box coordinates and superimposing them onto the images is given below.  "
      ],
      "metadata": {
        "id": "SiAYDrw0o1ml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2C0_NaZrlE6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import cv2\n",
        "\n",
        "bb = f'{val_data}/val_bboxes_resized.json'\n",
        "\n",
        "\n",
        "print('Sample from validation set with bounding box annotations\\n')\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "with open(bb, 'r') as j:\n",
        "    bb_kvasir = json.loads(j.read())\n",
        "\n",
        "    for i, (k,content) in enumerate(list(bb_kvasir.items())[0:3]):\n",
        "      fig.add_subplot(3, 1, i+1)\n",
        "      print(k,content)\n",
        "      im = np.array(Image.open(f'{val_data}/images/{k}.jpg'))\n",
        "\n",
        "      for i,box in enumerate(bb_kvasir[k]['bbox']):\n",
        "        #print json file line with information on bounding box\n",
        "        print((bb_kvasir[k]['bbox'][i]['xmin'],bb_kvasir[k]['bbox'][i]['ymin']), (bb_kvasir[k]['bbox'][i]['xmax'],bb_kvasir[k]['bbox'][i]['ymax']))\n",
        "\n",
        "        cv2.rectangle(im, (bb_kvasir[k]['bbox'][i]['xmin'],bb_kvasir[k]['bbox'][i]['ymin']), (bb_kvasir[k]['bbox'][i]['xmax'],bb_kvasir[k]['bbox'][i]['ymax']), [255,255,255], thickness=3)\n",
        "      plt.axis('off')\n",
        "      plt.imshow(im)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS5tAPam5H1y"
      },
      "source": [
        "#Training a segmentation model in Pytorch\n",
        "In the following examples of defining and training segmentation models in Pytorch will be given. Including how to create a dataset and dataloaders for efficient training and setting up a training loop for model parameter updates using a training set and running validation on a seperate validation set to monitor the model's ability to generalize outside the training data.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqRcBFevvKw5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import tqdm\n",
        "\n",
        "\n",
        "train_images = sorted([os.path.join('kvasir_polyps/train/images',image)for image in os.listdir('kvasir_polyps/train/images')])\n",
        "\n",
        "train_labels = sorted([os.path.join('kvasir_polyps/train/labels',label)for label in os.listdir('kvasir_polyps/train/labels')])\n",
        "\n",
        "print(f'number of train images: {len(train_images)} and labels: {len(train_images)}')\n",
        "\n",
        "#check if images and labels match\n",
        "print(f'train image 0-4: {train_images[0:4]}')\n",
        "print(f'train label 0-4: {train_labels[0:4]}')\n",
        "\n",
        "val_images = sorted([os.path.join('kvasir_polyps/val/images',image)for image in os.listdir('kvasir_polyps/val/images')])\n",
        "\n",
        "val_labels = sorted([os.path.join('kvasir_polyps/val/labels',label)for label in os.listdir('kvasir_polyps/val/labels')])\n",
        "\n",
        "print(f'number of val images: {len(val_images)} and labels: {len(val_images)}')\n",
        "\n",
        "#check if images and labels match\n",
        "print(f'val image 0-4: {val_images[0:4]}')\n",
        "print(f'val label 0-4: {val_labels[0:4]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataset class to fetch data from the downloaded archive"
      ],
      "metadata": {
        "id": "n-Y4KttAt45P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LklifKcnFVNb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  def __init__(self, imagePaths, maskPaths, transforms):\n",
        "    # store the image and mask filepaths, and augmentation\n",
        "    # transforms\n",
        "    self.imagePaths = imagePaths\n",
        "    self.maskPaths = maskPaths\n",
        "    self.transforms = transforms\n",
        "  def __len__(self):\n",
        "    # return the number of total samples contained in the dataset\n",
        "    return len(self.imagePaths)\n",
        "  def __getitem__(self, idx):\n",
        "    # grab the image path from the current index\n",
        "    imagePath = self.imagePaths[idx]\n",
        "    # load the image from disk, swap its channels from BGR to RGB,\n",
        "    # and read the associated mask from disk in grayscale mode\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.imread(self.maskPaths[idx], 0)\n",
        "\n",
        "    # check to see if we are applying any transformations\n",
        "    if self.transforms is not None:\n",
        "      # apply the transformations to both image and its mask\n",
        "      image = self.transforms(image)\n",
        "\n",
        "    mask = torch.from_numpy(mask[np.newaxis,:]).to(torch.float32)\n",
        "\n",
        "    # return a tuple of the image and its mask\n",
        "    return (image, mask)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Unet segmentation model"
      ],
      "metadata": {
        "id": "0jbBlfnZv4zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define a segmentation model class using the torch nn api. The model is based on the [Unet architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) which is a seminal architecture for biomedical image segmentation. Despite it beeing somewhat old (~2015) it is still a good baseline for segmentation and many of the more recent SOTA segmentation models still use the encoder-decoder structure with skip-connections introduced by this work.\n",
        "\n",
        "The below model doesn't follow the original configuration to a key, but is fundamentally the same architecture.\n",
        "\n",
        "**note:**\n",
        "Even this rather basic architecture has a relative large number of parameters, and the resolution of the images used for training is relatively high. Training this model on this data will take a very long time on a CPU. Colab provides some free GPU compute but not a lot. If running this notebook and future deep learning work in Colab it may be a good idea to invest in some compute unit in [colab pro](https://colab.research.google.com/signup).\n",
        "\n",
        "To enable GPU compute in this notebook go to Runtime --> change runtime type and choose accelarator for the notebook."
      ],
      "metadata": {
        "id": "aqgqRgBsuJvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEw2Ye9tvZiY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu\n",
        "from torchsummary import summary\n",
        "\n",
        "#Example u-net encoder-decoder model implementation for n-class segmentation\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(encoder_block, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.block(x)\n",
        "      return x\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(decoder_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, n_class=1,filters=[64,128,256,512]):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "\n",
        "        self.e1 = encoder_block(in_channels,filters[0])\n",
        "        self.e2 = encoder_block(filters[0],filters[1])\n",
        "        self.e3 = encoder_block(filters[1],filters[2])\n",
        "        self.e4 = encoder_block(filters[2],filters[3])\n",
        "\n",
        "\n",
        "\n",
        "        self.d1 = decoder_block(filters[3],filters[2])\n",
        "        self.d2 = decoder_block(filters[2]+filters[2],filters[1])\n",
        "        self.d3 = decoder_block(filters[1]+filters[1],filters[1])\n",
        "\n",
        "        self.seg_block = nn.Sequential(nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            nn.Conv2d(filters[1]+filters[0],filters[0],kernel_size=3,padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0],n_class,kernel_size=1))\n",
        "        # the upsampling layer 'nn.UpsamplingBilinear2d' could be replaced with\n",
        "        # a transpose convolution 'nn.ConvTranspose2d'. This will add more\n",
        "        # parameters to the model but may lead to different/better results\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        x1 = self.e1(x)\n",
        "        x2 = self.e2(x1)\n",
        "        x3 = self.e3(x2)\n",
        "        x4 = self.e4(x3)\n",
        "\n",
        "        #decode\n",
        "        x5 = self.d1(x4)\n",
        "\n",
        "        x5 = torch.cat([x5,x3],1)\n",
        "\n",
        "        x6 = self.d2(x5)\n",
        "\n",
        "        x6 = torch.cat([x6,x2],1)\n",
        "\n",
        "        x7 = self.d3(x6)\n",
        "\n",
        "        x7 = torch.cat([x7,x1],1)\n",
        "\n",
        "        x7 = self.seg_block(x7)\n",
        "\n",
        "\n",
        "        return x7\n",
        "\n",
        "#print a summary of the model\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "Segmentation_model = UNet(n_class=1).to(DEVICE)\n",
        "\n",
        "summary(Segmentation_model,(3,512,512),1)\n",
        "\n",
        "#delete from memory\n",
        "del Segmentation_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configure som settings for this model and training setup"
      ],
      "metadata": {
        "id": "O8-SL6KT1g3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgio96-7AGWV"
      },
      "outputs": [],
      "source": [
        "# define the number of channels in the input, number of classes,\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# initialize learning rate, number of epochs to train for, and the\n",
        "# batch size\n",
        "INIT_LR = 0.0001\n",
        "NUM_EPOCHS = 40\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# define threshold to filter weak predictions\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "transformations = transforms.Compose([transforms.ToPILImage(),\n",
        "                                 transforms.ToTensor()])\n",
        "\n",
        "# create the train and test datasets\n",
        "trainDS = Dataset(imagePaths=train_images, maskPaths=train_labels,\n",
        "\ttransforms=transformations)\n",
        "valDS = Dataset(imagePaths=val_images, maskPaths=val_labels,\n",
        "    transforms=transformations)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sh2JAQAzhsi"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() #run this command to release GPU memory occupied by model/data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function for model training. The function takes the model, training and validation data, loss function and optimizer, number of epoch, device on which to model i training (cpu/gpu) as well as a string name under which the best performing model is saved and can later be used for inference on test data.\n",
        "\n",
        "After each iteration the loss function calculates a loss/cost quantifying the models performance/ability to solve the problem (segmenting polyps). In this example we use the [binary crossentropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) with logits. This function takes the output of the last layer of the model at each pixel position and squeezes them between [0,1] signifying a probability of each pixel being a part of a polyp (p=1 if the model is absolutely confident in the pixel beeing a polyp) og background (p=0 if the model is absolutely confident in the pixel being background/normal pixel). The total loss is calculated as the mean across all pixels in the image/batch of images. The theoretical minimum loss is equal to zero if we have a perfect model.\n",
        "\n",
        "The optimizer is an algorithm for minimizing the loss of the model by calculating gradient of the model parameters with respect to the loss. This gradient represents each parameters influence on the loss (positive or negative) and is used to update parameters by subtracting the gradient multiplied by a factor (learning rate/step size) thus (idealy) minimizing the loss of the model. This method of optimization is refered to as gradient decent and [diffenrent optimization algorithms exist](https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#) for this purpose. The learning rate should be a small value (0.001 - 0.00001) to avoid noisy updates. If the learning rate is too high, minimizing the loss may fail due to noizy updates from large step sizes, or lead to overfitting (model does not generalize outside the training data). Consequently, if the learning rate is too low the parameter value updates may be too small to have an meaningfull effect on the loss. Learning the optimal set of parameteres may take exceptionally long or the model gets \"stuck\" in the \"loss landscape\" not reaching a (somewhat) optimal set of parameteres. The learning rate is a (crucial) hyperparameter (it can't be learned but found trough experiments/searching). To determine whether we have found a good learning rate we monitor the training loss during training (\"if it gets lower = good\") as well as the validation loss (\"does it get lower as well = good. Does it rise/diverge signicantly from training loss = bad\")      "
      ],
      "metadata": {
        "id": "v-zdutzr2TSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Segmentation model performance is often measured using metrics describing the\n",
        "similarity between the ground truth mask and the mask generated by the model.\n",
        "One such metric is the Dice metric or dice score. To calculate the dice score\n",
        "of our models we use the torch metrics library\n",
        "\"\"\"\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "pXCCP5p840v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gHaKrqINrtf"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.nn.functional import sigmoid\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.functional import dice\n",
        "\n",
        "def train_function(model,trainDS,valDS,lossFunction=BCEWithLogitsLoss(),\n",
        "                   optimizer = Adam,learningRate = 0.001,\n",
        "                   epochs=40,batchSize=4, metric = dice, device=\"cpu\",modelSaveName='model'):\n",
        "\n",
        "  model.to(device)\n",
        "  #metric.to(device)\n",
        "\n",
        "  # calculate steps per epoch for training and test set\n",
        "  trainSteps = len(trainDS) // batchSize\n",
        "  valSteps = len(valDS) // batchSize\n",
        "\n",
        "  # create the training and test data loaders\n",
        "  trainLoader = DataLoader(trainDS, shuffle=True,\n",
        "\t  batch_size=batchSize,\n",
        "\t  num_workers=os.cpu_count())\n",
        "  valLoader = DataLoader(valDS, shuffle=False,\n",
        "\t  batch_size=batchSize,\n",
        "\t  num_workers=os.cpu_count())\n",
        "\n",
        "  opt = optimizer(model.parameters(), lr=learningRate)\n",
        "\n",
        "  bestValLoss = 1e10\n",
        "\n",
        "  # initialize a dictionary to store training history\n",
        "  H = {\"train_loss\": [], \"val_loss\": [], \"train_dice\": [], \"val_dice\": []}\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"EPOCH: {e + 1}/{NUM_EPOCHS}\")\n",
        "    # set the model in training mode\n",
        "    model.train()\n",
        "    # initialize the total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalValLoss = 0\n",
        "    totalTrainDice = 0\n",
        "    totalValDice = 0\n",
        "    # loop over the training set\n",
        "    print('Train phase:')\n",
        "    for (i, (x, y)) in tqdm.tqdm(enumerate(trainLoader)):\n",
        "  \t  # send the input to the device\n",
        "      (x, y) = (x.to(device), y.to(device))\n",
        "      # perform a forward pass and calculate the training loss\n",
        "      pred = model(x)\n",
        "      loss = lossFunc(pred, y)\n",
        "      dice_score = metric(sigmoid(pred),y.int())\n",
        "      #print(dice_score)\n",
        "      totalTrainDice += dice_score\n",
        "      # first, zero out any previously accumulated gradients, then\n",
        "      # perform backpropagation, and then update model parameters\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      # add the loss to the total training loss so far\n",
        "      totalTrainLoss += loss\n",
        "    # switch off autograd\n",
        "    print('\\n')\n",
        "    with torch.no_grad():\n",
        "      # set the model in evaluation mode\n",
        "      model.eval()\n",
        "      # loop over the validation set\n",
        "      print('val phase:')\n",
        "      for (i,(x, y)) in tqdm.tqdm(enumerate(valLoader)):\n",
        "        # send the input to the device\n",
        "        (x, y) = (x.to(device), y.to(device))\n",
        "        # make the predictions and calculate the validation loss\n",
        "        pred = model(x)\n",
        "        totalValLoss += lossFunc(pred, y)\n",
        "        dice_score = metric(sigmoid(pred),y.int())\n",
        "        totalValDice += dice_score\n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = totalTrainLoss / trainSteps\n",
        "    avgValLoss = totalValLoss / valSteps\n",
        "\n",
        "    avgTrainDice = totalTrainDice / trainSteps\n",
        "    avgValDice = totalValDice / valSteps\n",
        "    # update our training history\n",
        "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "    H[\"train_dice\"].append(avgTrainDice.cpu().detach().numpy())\n",
        "    H[\"val_dice\"].append(avgValDice.cpu().detach().numpy())\n",
        "    # print the model training and validation information\n",
        "\n",
        "    print(f\"Train loss: {avgTrainLoss:.6f}, Val loss: {avgValLoss:.4f}\")\n",
        "    print(f\"Train dice: {avgTrainDice:.3f}, Val dice: {avgValDice:.3f}\")\n",
        "\n",
        "    #save the model performing best on the validation set\n",
        "    if avgValLoss < bestValLoss:\n",
        "\n",
        "      torch.save({'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss': avgValLoss},\n",
        "\t          f'{modelSaveName}.pth')\n",
        "      bestValLoss = avgValLoss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 5))\n",
        "\n",
        "    im = np.moveaxis(x[0].detach().cpu().numpy(),0,-1)\n",
        "    mask = np.moveaxis(np.stack([np.squeeze(y[0].detach().cpu().numpy(),axis=0)]*3,axis=0),0,-1)\n",
        "    out = np.moveaxis(np.stack([np.squeeze(sigmoid(pred[0]).detach().cpu().numpy(),axis=0)]*3,axis=0),0,-1)\n",
        "    print(f'min polyp pixel probability: {np.min(out)}')\n",
        "    print(f'max polyp pixel probability: {np.max(out)}')\n",
        "    bin = out.copy()\n",
        "    bin[bin >= THRESHOLD] = 1\n",
        "    bin[bin < THRESHOLD] = 0\n",
        "\n",
        "    ax[0].imshow(im)\n",
        "    ax[0].set_title(f\"Input Image\")\n",
        "\n",
        "    ax[1].imshow(mask)\n",
        "    ax[1].set_title(f\"Reference Mask\")\n",
        "\n",
        "    ax[2].imshow(out)\n",
        "    ax[2].set_title(f\"Output Probability Map\")\n",
        "\n",
        "    ax[3].imshow(bin)\n",
        "    ax[3].set_title(f\"Model segmentation\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "  return H\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model\n",
        "Here we can use the training function to optimize the model. The function returns a \"history\" (per epoch validation and training loss). we could plot this to get an idea about the models ability to generalize (overfitting), if the learning rate is apropiate, or whether the model is capable of solving the problem. If it can't be (over)fittet to the training set, it unlikely that we are going to find an optimal solution using this model. It is a nice, simple way to minitor training and eyeball hyperparameter settings before performing a more extensive search.\n",
        "\n",
        "In colab you can adjust runtime settings to e.g. train using a GPU. This will decrease training time significantly compared to running on the cpu. To do this, go to runtime at the top of the page click on runtime type and select a GPU under hardware accelerator. Either way, training may take a while. The intend of the notebook i to give an example of using deep learning models for biomedical image segmentation. So in this next part you can just run the code as is or do experiments with different parameters to observe the effects, and then move on when it makes sense. The task is not to achieve a fully optimized model.  "
      ],
      "metadata": {
        "id": "gvBwMTiucD6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the device to be used for training and evaluation\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# initialize our UNet model\n",
        "unet = UNet(in_channels=NUM_CHANNELS,n_class=NUM_CLASSES)\n",
        "# initialize loss function and optimizer\n",
        "lossFunc = BCEWithLogitsLoss(reduction=\"mean\")\n",
        "\"\"\"\n",
        "train the model. With the default parameters in this notebook\n",
        "it may take some time (~10 epochs)\n",
        "before the model begins to assign high probability for polyp\n",
        "to any pixels in the chosen validation smaple\n",
        "\"\"\"\n",
        "\n",
        "history = train_function(unet,trainDS,valDS,lossFunction=lossFunc,optimizer=Adam,\n",
        "               learningRate = INIT_LR,\n",
        "               epochs=NUM_EPOCHS,\n",
        "               batchSize=BATCH_SIZE,\n",
        "               device=DEVICE,\n",
        "               modelSaveName='unet')"
      ],
      "metadata": {
        "id": "FIrkdVkaVff9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache() #run this command to release GPU memory"
      ],
      "metadata": {
        "id": "ua18_OPmroPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make use of the model\n",
        "Once the model is trained we can use it to perform segmentation of new images by reinitalizing it and loading the optimized parameters.   "
      ],
      "metadata": {
        "id": "Z7UMunSeeqWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#redefine model and load best parameters to run inference on test data\n",
        "#This is left as an exercise\n",
        "unet = UNet(in_channels=NUM_CHANNELS,n_class=NUM_CLASSES)\n",
        "checkpoint = torch.load('unet.pth')\n",
        "\n",
        "unet.load_state_dict(checkpoint['model_state_dict'])\n"
      ],
      "metadata": {
        "id": "cPiTzK75cGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean up before continuing\n",
        "try:\n",
        "  del unet\n",
        "except:\n",
        "  pass\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "EQnG2SPatyLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##More advanced + pre-trained models\n",
        "Depending on the data, it might be the case that a relative simple model such as Unet won't be able to solve the problem (e.g. polyp segmentation). In this case, we may need to increase the complexity of the model by adding more layers, or reconfigure layers to include components such a [residual connections](https://arxiv.org/pdf/1512.03385.pdf) or [batch normalization](https://arxiv.org/pdf/1502.03167.pdf). It may also be the case that we simply don't have enough data to obtain a good, generalised model (regardless of complexity). In this case we can make use of pretrained models. Pretrainng is a method were the model is first trained one a larger dataset and the parameters obtained in this setting are then used as initial values and finetuned on a downstream task. For encoder-decoder type models such a Unet, a common approach is the use a model trained for classification on the [ImageNet dataset](https://www.image-net.org/index.php) as the encoder \"backbone\". A caveat with increasing complexity and making use of pretrained models is that this may lead to problems with overfitting. The best way compat overfitting is to train on more (representative) data. But in the medical domain this may not always be possible or atleast very costly. An alternative solution is to make use of relurazation techniques such as data augmentation and dropout ([as in the AlexNet paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)). It is also a good idea to decrease the learning rate compared to when training on randomly initialized weights.\n",
        "\n",
        "Below we can try to solve the same polyp segmentation problem as above, but using a unet with a pretrained backbone. To illustrate the difference between a model with random initialized parameters (or weights), we plot the some of the \"filters\" from the first layer of the model and observe how these have been optimized to react to different features in the input images during the pretraining process.  "
      ],
      "metadata": {
        "id": "E8SarkOaHqeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the Segmentation-models-pytorch library to access predefined segmentation models with pretrained weights\n",
        "#https://segmentation-modelspytorch.readthedocs.io/en/latest/#\n",
        "!pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "id": "ClxkRwIrqZtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izEvv06glgOg"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "model = smp.Unet(encoder_name=\"resnet18\",encoder_weights='imagenet',encoder_depth=4,decoder_channels = [256, 128, 64,32]).to(DEVICE)\n",
        "\n",
        "summary(model,(3,512,512),1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_kernels(tensor, num_cols=6):\n",
        "    if not tensor.ndim==4:\n",
        "        raise Exception(\"assumes a 4D tensor\")\n",
        "    #if not tensor.shape[-1]==3:\n",
        "    #    raise Exception(\"last dim needs to be 3 to plot\")\n",
        "    num_kernels = tensor.shape[0]\n",
        "    num_rows = 1+ num_kernels // num_cols\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    for i in range(tensor.shape[0]):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        ax1.imshow(np.moveaxis(tensor[i],0,-1)*255)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "edy9I3KqlGH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize unet model with resnet encoder without pretrained weights and plot the weights of the first convolution layer with 7x7 filters\n",
        "\n",
        "model_random = smp.Unet(encoder_name=\"resnet18\",encoder_weights=None,encoder_depth=4,decoder_channels = [256, 128, 64,32]).to(DEVICE)\n",
        "\n",
        "mm = model_random.double()\n",
        "body_model = [i for i in mm.children()][0]\n",
        "body_mm = body_model.double()\n",
        "body_layers =  [i for i in body_mm.children()][0]\n",
        "\n",
        "tensor = body_layers.weight.data.cpu().numpy()\n",
        "plot_kernels(tensor)\n"
      ],
      "metadata": {
        "id": "yyb1669Tk-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize unet model with resnet encoder with imagenet pretrained weights and plot the weights of the first convolution layer with 7x7 filters\n",
        "model_imagenet = smp.Unet(encoder_name=\"resnet18\",encoder_weights='imagenet',encoder_depth=4,decoder_channels = [256, 128, 64,32]).to(DEVICE)\n",
        "\n",
        "mm = model_imagenet.double()\n",
        "body_model = [i for i in mm.children()][0]\n",
        "body_mm = body_model.double()\n",
        "body_layers =  [i for i in body_mm.children()][0]\n",
        "\n",
        "tensor = body_layers.weight.data.cpu().numpy()\n",
        "plot_kernels(tensor)"
      ],
      "metadata": {
        "id": "5MOtdnpCzj1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "#Initialize unet model with resnet encoder with imagenet pretrained weights and plot the weights of the first convolution layer with 7x7 filters\n",
        "model_imagenet = smp.Unet(encoder_name=\"resnet18\",encoder_weights='imagenet',encoder_depth=4,decoder_channels = [256, 128, 64,32])\n",
        "\n",
        "# determine the device to be used for training and evaluation\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# initialize loss function and optimizer\n",
        "lossFunc = BCEWithLogitsLoss(reduction=\"mean\")\n",
        "\n",
        "hist = train_function(model_imagenet,trainDS,valDS,lossFunction=lossFunc,optimizer=Adam,\n",
        "               learningRate = INIT_LR*0.1,\n",
        "               epochs=NUM_EPOCHS,\n",
        "               batchSize=BATCH_SIZE,\n",
        "               device=DEVICE,\n",
        "               modelSaveName='smp-unet')"
      ],
      "metadata": {
        "id": "C2zR70qeryUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hC0CuFm8t1tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsCj71x2vdXZ"
      },
      "outputs": [],
      "source": [
        "#Run this cell to remove the data from the colab directory\n",
        "import shutil\n",
        "shutil.rmtree('kvasir_polyps')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}